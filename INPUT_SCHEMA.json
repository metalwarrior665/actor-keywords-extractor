{
    "title": "Keyword extractor input",
    "type": "object",
    "description": "Keyword extractor.",
    "schemaVersion": 1,
    "properties": {
        "startUrls": {
            "title": "Start URLs",
            "type": "array",
            "description": "A static list of URLs to scrape. To be able to add new URLs on the fly, enable the <b>Use request queue</b> option.<br><br>For details, see <a href='https://apify.com/apify/web-scraper#start-urls' target='_blank' rel='noopener'>Start URLs</a> in README.",
            "prefill": [
                { "url": "https://apify.com" }
            ],
            "editor": "requestListSources"
        },
        "useBrowser": {
            "title": "Use Browser",
            "type": "boolean",
            "description": "If on, it will use regular borwser for scraping.",
            "default": false
        },
        "keywords": {
            "title": "Keywords",
            "type": "array",
            "description": "List of keywords to search and count on every page",
            "editor": "stringList",
            "prefill": ["apify", "web", "scraping", "automation"]
        },
        "caseSensitive": {
            "title": "Case sensitive",
            "type": "boolean",
            "description": "If on, it will only match keywords with exact upper or lower case.",
            "default": false
        },
        "scanScripts": {
            "title": "Scan scripts",
            "type": "boolean",
            "description": "If on, it will also count keywords appearing inside scripts.",
            "default": false
        },
        "linkSelector": {
            "title": "Link selector",
            "type": "string",
            "description": "A CSS selector saying which links on the page (<code>&lt;a&gt;</code> elements with <code>href</code> attribute) shall be followed and added to the request queue. This setting only applies if <b>Use request queue</b> is enabled. To filter the links added to the queue, use the <b>Pseudo-URLs</b> setting.<br><br>If <b>Link selector</b> is empty, the page links are ignored.<br><br>For details, see <a href='https://apify.com/apify/web-scraper#link-selector' target='_blank' rel='noopener'>Link selector</a> in README.",
            "editor": "textfield",
            "prefill": "a[href]"
        },
        "pseudoUrls": {
            "title": "Pseudo-URLs",
            "type": "array",
            "description": "Specifies what kind of URLs found by <b>Link selector</b> should be added to the request queue. A pseudo-URL is a URL with regular expressions enclosed in <code>[]</code> brackets, e.g. <code>http://www.example.com/[.*]</code>. This setting only applies if the <b>Use request queue</b> option is enabled.<br><br>If <b>Pseudo-URLs</b> are omitted, the actor enqueues all links matched by the <b>Link selector</b>.<br><br>For details, see <a href='https://apify.com/apify/web-scraper#pseudo-urls' target='_blank' rel='noopener'>Pseudo-URLs</a> in README.",
            "editor": "pseudoUrls",
            "default": [],
            "prefill": [
                {
                    "purl": "https://apify.com/[.*]"
                }
            ]
        },
        "maxDepth": {
            "title": "Max depth",
            "type": "integer",
            "description": "How many links deep from the Start URLs do you want to crawl. Start URLs have depth 0.",
            "minimum": 0,
            "default": 5
        },
        "proxyConfiguration": {
            "title": "Proxy configuration",
            "type": "object",
            "description": "Specifies proxy servers that will be used by the scraper in order to hide its origin.<br><br>For details, see <a href='https://apify.com/apify/web-scraper#proxy-configuration' target='_blank' rel='noopener'>Proxy configuration</a> in README.",
            "prefill": { "useApifyProxy": false },
            "default": {},
            "editor": "proxy"
        },
        "maxPagesPerCrawl": {
            "title": "Max pages per run",
            "type": "integer",
            "description": "The maximum number of pages that the scraper will load. The scraper will stop when this limit is reached. It's always a good idea to set this limit in order to prevent excess platform usage for misconfigured scrapers. Note that the actual number of pages loaded might be slightly higher than this value.<br><br>If set to <code>0</code>, there is no limit.",
            "minimum": 0,
            "default": 100
        },
        "maxConcurrency": {
            "title": "Max concurrency",
            "type": "integer",
            "description": "Specified the maximum number of pages that can be processed by the scraper in parallel. The scraper automatically increases and decreases concurrency based on available system resources. This option enables you to set an upper limit, for example to reduce the load on a target website.",
            "minimum": 1,
            "default": 50
        },
        "retireInstanceAfterRequestCount": {
            "title": "Retire Instance After Request Count",
            "type": "integer",
            "description": "How often will the browser itself rotate. Pick higher for smaller consumption, pick less to rotate (test) more proxies",
            "minimum": 1,
            "default": 50
        },
        "useChrome": {
            "title": "Use Chrome",
            "type": "boolean",
            "description": "Only works for puppeteer type. Be careful that Chrome is not guaranteed to work with Puppeteer.",
            "default": false
        },
        "waitFor": {
            "title": "Wait for",
            "type": "string",
            "description": "Only works for puppeteer type. Will wait on each page. You can provide number in ms or a selector.",
            "editor": "textfield"
        }
    },
    "required": ["startUrls", "keywords"]
}
